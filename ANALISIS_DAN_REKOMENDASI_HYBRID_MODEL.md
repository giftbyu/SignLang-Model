# üéØ ANALISIS DAN OPTIMASI MODEL HYBRID DETEKSI BAHASA ISYARAT
## Integrasi NLP & Visual dengan EfficientNetB0 dan MediaPipe

---

## üìã **RINGKASAN EKSEKUTIF**

Repository ini mengimplementasikan sistem deteksi bahasa isyarat (SIBI/BISINDO) menggunakan pendekatan **hybrid multi-modal** yang menggabungkan:
1. **Visual Features**: EfficientNetB0 (CNN pre-trained)
2. **Geometric Features**: MediaPipe Landmarks (MLP)

### **Rekomendasi Utama:**
Untuk memaksimalkan model hybrid dengan integrasi NLP, kami merekomendasikan upgrade ke **multi-modal transformer architecture** yang menggabungkan:
- **Vision Transformer (ViT)** atau **EfficientNetV2** untuk visual
- **Transformer Encoder** untuk sequence modeling
- **Multi-Head Cross-Attention** untuk fusion NLP-Visual
- **Contrastive Learning** untuk alignment antar modalitas

---

## üîç **ANALISIS ARSITEKTUR SAAT INI**

### **1. Visual Branch (EfficientNetB0)**

#### ‚úÖ **Kelebihan:**
- Pre-trained ImageNet ‚Üí transfer learning yang baik
- Efisien secara komputasi (compound scaling)
- Global Average Pooling ‚Üí invariansi terhadap posisi

#### ‚ö†Ô∏è **Limitasi:**
- **Tidak optimal untuk sequence**: Bahasa isyarat adalah *temporal sequence*, bukan single frame
- **Ukuran input kecil** (128x128): EfficientNetB0 dirancang untuk 224x224+
- **Feature extraction statis**: Tidak menangkap gerakan dinamis

#### üí° **Rekomendasi Upgrade:**
```python
# UPGRADE 1: EfficientNetV2 dengan ukuran input optimal
base_model = tf.keras.applications.EfficientNetV2S(
    include_top=False,
    weights='imagenet',
    input_shape=(224, 224, 3),  # Ukuran optimal
    pooling='avg'
)

# UPGRADE 2: Tambahkan Temporal Modeling (untuk video sequence)
from tensorflow.keras.layers import LSTM, Bidirectional, TimeDistributed

# Untuk sequence of frames
visual_lstm = Bidirectional(LSTM(256, return_sequences=True))(frame_features)
visual_attention = tf.keras.layers.MultiHeadAttention(
    num_heads=8, 
    key_dim=64
)(visual_lstm, visual_lstm)
```

---

### **2. Geometric Branch (MediaPipe Landmarks)**

#### ‚úÖ **Kelebihan:**
- **Lightweight dan cepat**: Real-time inference
- **Invariansi lighting**: Tidak terpengaruh pencahayaan
- **Koordinat relatif**: Invariansi translasi

#### ‚ö†Ô∏è **Limitasi:**
- **MLP = tidak menangkap sequential dependency**: Bahasa isyarat memiliki temporal order
- **Flat vector**: Kehilangan struktur spasial antar landmark
- **No hand shape encoding**: Tidak memanfaatkan relasi geometris (jarak, sudut)

#### üí° **Rekomendasi Upgrade:**
```python
# UPGRADE 1: Graph Neural Network untuk relasi landmark
# Landmark tangan membentuk graph natural dengan koneksi antar jari

import tensorflow as tf
from spektral.layers import GCNConv, GlobalAttentionPool

def build_landmark_gcn(num_landmarks=21):
    # Adjacency matrix untuk hand skeleton
    adjacency = create_hand_skeleton_adjacency()  # 21x21
    
    # Input: (batch, num_landmarks, 3) untuk x,y,z
    landmark_input = Input(shape=(num_landmarks, 3))
    
    # GCN layers
    x = GCNConv(64, activation='relu')([landmark_input, adjacency])
    x = GCNConv(128, activation='relu')([x, adjacency])
    x = GlobalAttentionPool(256)(x)
    
    return Model(landmark_input, x, name='LandmarkGCN')

# UPGRADE 2: Tambahkan hand shape features
def extract_hand_features(landmarks):
    """
    Ekstrak fitur geometris tingkat tinggi:
    - Distances antar landmark key points
    - Angles antar jari
    - Palm orientation
    """
    # Contoh: Jarak ujung jari ke wrist
    finger_tips = landmarks[:, [4, 8, 12, 16, 20]]  # Thumb, Index, Middle, Ring, Pinky
    wrist = landmarks[:, 0:1]
    distances = tf.norm(finger_tips - wrist, axis=-1)
    
    # Contoh: Sudut antar jari
    vectors = finger_tips[:, 1:] - finger_tips[:, :-1]
    angles = tf.acos(tf.reduce_sum(
        vectors[:, :-1] * vectors[:, 1:], axis=-1
    ) / (tf.norm(vectors[:, :-1], axis=-1) * tf.norm(vectors[:, 1:], axis=-1) + 1e-6))
    
    return tf.concat([distances, angles], axis=-1)
```

---

### **3. Fusion Strategy (Simple Concatenation)**

#### ‚ö†Ô∏è **Limitasi:**
- **Late fusion**: Tidak ada interaksi antar modalitas selama feature extraction
- **Simple concatenation**: Tidak optimal untuk cross-modal reasoning
- **No attention**: Tidak bisa fokus pada modalitas yang lebih informatif

#### üí° **Rekomendasi Upgrade:**
```python
# UPGRADE 1: Multi-Head Cross-Attention Fusion
class CrossModalAttentionFusion(tf.keras.layers.Layer):
    def __init__(self, num_heads=8, key_dim=64, **kwargs):
        super().__init__(**kwargs)
        self.cross_attention_v2l = tf.keras.layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=key_dim, name='visual_to_landmark_attention'
        )
        self.cross_attention_l2v = tf.keras.layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=key_dim, name='landmark_to_visual_attention'
        )
        self.layer_norm_v = tf.keras.layers.LayerNormalization()
        self.layer_norm_l = tf.keras.layers.LayerNormalization()
        
    def call(self, visual_features, landmark_features):
        # Visual attends to landmark
        v2l = self.cross_attention_v2l(
            query=visual_features, 
            key=landmark_features, 
            value=landmark_features
        )
        visual_enhanced = self.layer_norm_v(visual_features + v2l)
        
        # Landmark attends to visual
        l2v = self.cross_attention_l2v(
            query=landmark_features, 
            key=visual_features, 
            value=visual_features
        )
        landmark_enhanced = self.layer_norm_l(landmark_features + l2v)
        
        # Concatenate enhanced features
        return tf.concat([visual_enhanced, landmark_enhanced], axis=-1)

# UPGRADE 2: Gated Fusion (belajar bobot optimal antar modalitas)
class GatedMultimodalFusion(tf.keras.layers.Layer):
    def __init__(self, hidden_dim=256, **kwargs):
        super().__init__(**kwargs)
        self.gate_v = Dense(hidden_dim, activation='sigmoid', name='visual_gate')
        self.gate_l = Dense(hidden_dim, activation='sigmoid', name='landmark_gate')
        self.transform_v = Dense(hidden_dim, name='visual_transform')
        self.transform_l = Dense(hidden_dim, name='landmark_transform')
        
    def call(self, visual_features, landmark_features):
        # Gated fusion: model learns which modality to trust
        v_transformed = self.transform_v(visual_features)
        l_transformed = self.transform_l(landmark_features)
        
        gate_v = self.gate_v(tf.concat([visual_features, landmark_features], axis=-1))
        gate_l = self.gate_l(tf.concat([visual_features, landmark_features], axis=-1))
        
        fused = gate_v * v_transformed + gate_l * l_transformed
        return fused
```

---

## üî§ **INTEGRASI NLP KE MODEL HYBRID**

### **Strategi 1: Sign Language Translation (Vision ‚Üí Text)**

Upgrade model menjadi **encoder-decoder architecture** untuk menghasilkan teks dari gesture:

```python
# ARSITEKTUR: Vision-Language Model untuk Sign Language Translation

class SignLanguageTranslator(tf.keras.Model):
    def __init__(self, vocab_size, max_seq_length=50):
        super().__init__()
        
        # ENCODER: Multi-modal (Vision + Landmarks)
        self.visual_encoder = build_visual_encoder()  # EfficientNetV2 + Temporal
        self.landmark_encoder = build_landmark_gcn()
        self.cross_modal_fusion = CrossModalAttentionFusion()
        
        # DECODER: Transformer untuk text generation
        self.embedding = tf.keras.layers.Embedding(vocab_size, 512)
        self.positional_encoding = PositionalEncoding(max_seq_length, 512)
        self.decoder_layers = [
            TransformerDecoderLayer(d_model=512, num_heads=8, dff=2048)
            for _ in range(6)
        ]
        self.final_layer = Dense(vocab_size)
        
    def call(self, inputs, training=False):
        visual_input, landmark_input, text_input = inputs
        
        # Encode multi-modal input
        visual_features = self.visual_encoder(visual_input, training=training)
        landmark_features = self.landmark_encoder(landmark_input, training=training)
        
        # Cross-modal fusion
        fused_features = self.cross_modal_fusion(visual_features, landmark_features)
        
        # Decode to text
        text_embeddings = self.embedding(text_input)
        text_embeddings = self.positional_encoding(text_embeddings)
        
        for decoder_layer in self.decoder_layers:
            text_embeddings = decoder_layer(
                text_embeddings, 
                encoder_output=fused_features,
                training=training
            )
        
        output = self.final_layer(text_embeddings)
        return output

# TRAINING dengan Teacher Forcing
def train_step(model, visual, landmarks, target_text):
    with tf.GradientTape() as tape:
        # Shift target untuk teacher forcing
        decoder_input = target_text[:, :-1]
        decoder_target = target_text[:, 1:]
        
        predictions = model([visual, landmarks, decoder_input], training=True)
        loss = tf.keras.losses.sparse_categorical_crossentropy(
            decoder_target, predictions, from_logits=True
        )
        
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss
```

### **Strategi 2: Contrastive Vision-Language Pre-training (CLIP-style)**

Latih model dengan **contrastive learning** untuk align visual dan text embeddings:

```python
# ARSITEKTUR: CLIP-style Contrastive Learning untuk Sign Language

class SignLanguageCLIP(tf.keras.Model):
    def __init__(self, embedding_dim=512):
        super().__init__()
        
        # Vision Encoder (Image/Video + Landmarks)
        self.visual_encoder = build_visual_encoder()
        self.landmark_encoder = build_landmark_gcn()
        self.vision_projection = Dense(embedding_dim, name='vision_proj')
        
        # Text Encoder (untuk label/caption)
        self.text_encoder = tf.keras.layers.Embedding(vocab_size, 512)
        self.text_transformer = TransformerEncoder(num_layers=6, d_model=512)
        self.text_projection = Dense(embedding_dim, name='text_proj')
        
    def encode_vision(self, visual_input, landmark_input):
        v = self.visual_encoder(visual_input)
        l = self.landmark_encoder(landmark_input)
        fused = tf.concat([v, l], axis=-1)
        return self.vision_projection(fused)
    
    def encode_text(self, text_input):
        embeddings = self.text_encoder(text_input)
        encoded = self.text_transformer(embeddings)
        # Mean pooling
        pooled = tf.reduce_mean(encoded, axis=1)
        return self.text_projection(pooled)

# CONTRASTIVE LOSS (InfoNCE)
def contrastive_loss(vision_embeddings, text_embeddings, temperature=0.07):
    # Normalize embeddings
    vision_embeddings = tf.nn.l2_normalize(vision_embeddings, axis=-1)
    text_embeddings = tf.nn.l2_normalize(text_embeddings, axis=-1)
    
    # Compute similarity matrix
    logits = tf.matmul(vision_embeddings, text_embeddings, transpose_b=True) / temperature
    
    # Labels: diagonal elements are positive pairs
    batch_size = tf.shape(logits)[0]
    labels = tf.range(batch_size)
    
    # Symmetric loss (vision‚Üítext dan text‚Üívision)
    loss_v2t = tf.keras.losses.sparse_categorical_crossentropy(
        labels, logits, from_logits=True
    )
    loss_t2v = tf.keras.losses.sparse_categorical_crossentropy(
        labels, tf.transpose(logits), from_logits=True
    )
    
    return (loss_v2t + loss_t2v) / 2

# TRAINING PIPELINE
def train_clip_style(model, dataset, epochs=100):
    optimizer = tf.keras.optimizers.Adam(1e-4)
    
    for epoch in range(epochs):
        for batch in dataset:
            visual, landmarks, text = batch
            
            with tf.GradientTape() as tape:
                vision_emb = model.encode_vision(visual, landmarks)
                text_emb = model.encode_text(text)
                loss = contrastive_loss(vision_emb, text_emb)
            
            gradients = tape.gradient(loss, model.trainable_variables)
            optimizer.apply_gradients(zip(gradients, model.trainable_variables))

# INFERENCE: Zero-shot classification
def predict(model, visual_input, landmark_input, text_candidates):
    """
    Args:
        text_candidates: List of possible text descriptions
    Returns:
        Best matching text for the input gesture
    """
    vision_emb = model.encode_vision(visual_input, landmark_input)
    
    # Encode all text candidates
    text_embeddings = [model.encode_text(text) for text in text_candidates]
    text_embeddings = tf.stack(text_embeddings)
    
    # Compute similarity scores
    vision_emb = tf.nn.l2_normalize(vision_emb, axis=-1)
    text_embeddings = tf.nn.l2_normalize(text_embeddings, axis=-1)
    
    similarities = tf.matmul(vision_emb, text_embeddings, transpose_b=True)
    best_match_idx = tf.argmax(similarities[0])
    
    return text_candidates[best_match_idx], similarities[0]
```

### **Strategi 3: Multi-Task Learning (Classification + Translation)**

Kombinasi task classification dan text generation dalam satu model:

```python
class MultiTaskSignLanguageModel(tf.keras.Model):
    def __init__(self, num_classes, vocab_size):
        super().__init__()
        
        # Shared encoder
        self.shared_visual_encoder = build_visual_encoder()
        self.shared_landmark_encoder = build_landmark_gcn()
        self.fusion = CrossModalAttentionFusion()
        
        # Task 1: Classification head
        self.classifier = tf.keras.Sequential([
            Dense(512, activation='relu'),
            Dropout(0.5),
            Dense(num_classes, activation='softmax', name='classification_output')
        ])
        
        # Task 2: Translation head (text generation)
        self.text_decoder = TransformerDecoder(vocab_size=vocab_size)
        
    def call(self, inputs, training=False):
        visual, landmarks, text_input = inputs
        
        # Shared feature extraction
        v_features = self.shared_visual_encoder(visual, training=training)
        l_features = self.shared_landmark_encoder(landmarks, training=training)
        fused = self.fusion(v_features, l_features)
        
        # Task 1: Classification
        class_logits = self.classifier(fused, training=training)
        
        # Task 2: Text generation
        text_logits = self.text_decoder(
            text_input, 
            encoder_output=fused, 
            training=training
        )
        
        return {
            'classification': class_logits,
            'translation': text_logits
        }

# MULTI-TASK LOSS
def multi_task_loss(y_true_class, y_true_text, y_pred, alpha=0.5):
    """
    Args:
        alpha: Weight untuk balancing kedua loss (0.5 = equal weight)
    """
    # Classification loss
    loss_class = tf.keras.losses.sparse_categorical_crossentropy(
        y_true_class, y_pred['classification']
    )
    
    # Translation loss
    loss_translation = tf.keras.losses.sparse_categorical_crossentropy(
        y_true_text, y_pred['translation'], from_logits=True
    )
    
    # Combined loss
    total_loss = alpha * loss_class + (1 - alpha) * loss_translation
    return total_loss
```

---

## üéØ **REKOMENDASI DATASET DAN TRAINING**

### **1. Dataset Requirements untuk NLP Integration**

Untuk integrasi NLP, Anda memerlukan:

#### **A. Sign-to-Text Pairs**
```
Contoh struktur dataset:
{
    "video_path": "SIBI_A_001.mp4",
    "frames": [...],  # Sequence of frames
    "landmarks": [...],  # Sequence of landmarks
    "label": "A",  # Class label
    "text": "Huruf A dalam bahasa isyarat SIBI",  # Description
    "sentence": "Aku ingin belajar"  # Untuk continuous sign language
}
```

#### **B. Data Augmentation untuk Multi-Modal**
```python
# Augmentasi Visual
data_augmentation_visual = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
    layers.RandomContrast(0.2),
    layers.RandomBrightness(0.2),
])

# Augmentasi Landmarks (geometric transformations)
def augment_landmarks(landmarks, training=True):
    if not training:
        return landmarks
    
    # Random rotation
    angle = tf.random.uniform([], -15, 15) * np.pi / 180
    rotation_matrix = tf.stack([
        [tf.cos(angle), -tf.sin(angle), 0],
        [tf.sin(angle), tf.cos(angle), 0],
        [0, 0, 1]
    ])
    landmarks_rotated = tf.matmul(landmarks, rotation_matrix)
    
    # Random scaling
    scale = tf.random.uniform([], 0.9, 1.1)
    landmarks_scaled = landmarks_rotated * scale
    
    # Add small noise
    noise = tf.random.normal(tf.shape(landmarks_scaled), stddev=0.01)
    landmarks_augmented = landmarks_scaled + noise
    
    return landmarks_augmented
```

### **2. Training Strategy**

#### **A. Progressive Training (Recommended)**
```python
# STAGE 1: Pre-train visual encoder (EfficientNet)
# - Freeze: None
# - Epochs: 20
# - Task: Classification saja
# - Learning Rate: 1e-3

# STAGE 2: Pre-train landmark encoder
# - Freeze: Visual encoder
# - Epochs: 20
# - Task: Classification dengan landmark saja
# - Learning Rate: 1e-3

# STAGE 3: Joint training dengan frozen encoders
# - Freeze: Visual + Landmark encoders
# - Epochs: 30
# - Task: Classification dengan fusion
# - Learning Rate: 1e-4

# STAGE 4: Fine-tune end-to-end
# - Freeze: None (unfreeze all)
# - Epochs: 50
# - Task: Multi-task (Classification + Translation)
# - Learning Rate: 1e-5

def progressive_training_pipeline():
    # Stage 1
    model.visual_encoder.trainable = True
    model.landmark_encoder.trainable = False
    train(model, epochs=20, lr=1e-3, task='classification_only')
    
    # Stage 2
    model.visual_encoder.trainable = False
    model.landmark_encoder.trainable = True
    train(model, epochs=20, lr=1e-3, task='classification_only')
    
    # Stage 3
    model.visual_encoder.trainable = False
    model.landmark_encoder.trainable = False
    model.fusion_layer.trainable = True
    model.classifier.trainable = True
    train(model, epochs=30, lr=1e-4, task='classification')
    
    # Stage 4: Full fine-tuning
    model.trainable = True
    train(model, epochs=50, lr=1e-5, task='multi_task')
```

#### **B. Advanced Training Techniques**

```python
# 1. GRADIENT ACCUMULATION (untuk batch size besar pada GPU terbatas)
def train_with_gradient_accumulation(model, dataset, accumulation_steps=4):
    optimizer = tf.keras.optimizers.Adam(1e-4)
    
    for epoch in range(epochs):
        accumulated_gradients = [
            tf.Variable(tf.zeros_like(var), trainable=False) 
            for var in model.trainable_variables
        ]
        
        for step, batch in enumerate(dataset):
            with tf.GradientTape() as tape:
                predictions = model(batch, training=True)
                loss = compute_loss(batch, predictions)
                loss = loss / accumulation_steps
            
            # Accumulate gradients
            gradients = tape.gradient(loss, model.trainable_variables)
            for i, grad in enumerate(gradients):
                accumulated_gradients[i].assign_add(grad)
            
            # Apply accumulated gradients every N steps
            if (step + 1) % accumulation_steps == 0:
                optimizer.apply_gradients(
                    zip(accumulated_gradients, model.trainable_variables)
                )
                # Reset accumulated gradients
                for grad_var in accumulated_gradients:
                    grad_var.assign(tf.zeros_like(grad_var))

# 2. MIXED PRECISION TRAINING (untuk training lebih cepat)
from tensorflow.keras import mixed_precision
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

# 3. LEARNING RATE SCHEDULING
lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(
    initial_learning_rate=1e-3,
    first_decay_steps=1000,
    t_mul=2.0,
    m_mul=0.9,
    alpha=1e-6
)
optimizer = tf.keras.optimizers.Adam(lr_schedule)

# 4. LABEL SMOOTHING (mencegah overfitting)
def label_smoothing_loss(y_true, y_pred, smoothing=0.1):
    num_classes = tf.shape(y_pred)[-1]
    y_true_smooth = y_true * (1 - smoothing) + smoothing / tf.cast(num_classes, tf.float32)
    return tf.keras.losses.categorical_crossentropy(y_true_smooth, y_pred)
```

---

## üìä **EVALUASI MODEL HYBRID**

### **Metrics untuk Multi-Modal Model**

```python
class MultiModalMetrics:
    def __init__(self, num_classes, vocab_size):
        self.num_classes = num_classes
        self.vocab_size = vocab_size
        
        # Classification metrics
        self.classification_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()
        self.top5_accuracy = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)
        self.confusion_matrix = tf.zeros((num_classes, num_classes))
        
        # Translation metrics
        self.bleu_score = []  # Compute dengan sacrebleu library
        self.perplexity = []
        
        # Cross-modal metrics
        self.vision_text_alignment = []  # Cosine similarity
        
    def update(self, y_true_class, y_true_text, predictions):
        # Classification
        class_preds = predictions['classification']
        self.classification_accuracy.update_state(y_true_class, class_preds)
        self.top5_accuracy.update_state(y_true_class, class_preds)
        
        # Translation (BLEU score)
        text_preds = predictions['translation']
        text_preds_decoded = decode_text(text_preds)  # Convert logits to text
        y_true_text_decoded = decode_text(y_true_text)
        
        from sacrebleu import corpus_bleu
        bleu = corpus_bleu(text_preds_decoded, [y_true_text_decoded])
        self.bleu_score.append(bleu.score)
        
    def result(self):
        return {
            'classification_accuracy': self.classification_accuracy.result(),
            'top5_accuracy': self.top5_accuracy.result(),
            'bleu_score': np.mean(self.bleu_score),
            'perplexity': np.exp(np.mean(self.perplexity))
        }

# Evaluate cross-modal alignment
def evaluate_cross_modal_alignment(model, test_dataset):
    """
    Mengukur seberapa baik visual dan text features teralign
    """
    vision_embeddings = []
    text_embeddings = []
    
    for batch in test_dataset:
        v_emb = model.encode_vision(batch['visual'], batch['landmarks'])
        t_emb = model.encode_text(batch['text'])
        
        vision_embeddings.append(v_emb)
        text_embeddings.append(t_emb)
    
    vision_embeddings = tf.concat(vision_embeddings, axis=0)
    text_embeddings = tf.concat(text_embeddings, axis=0)
    
    # Compute cosine similarity
    vision_norm = tf.nn.l2_normalize(vision_embeddings, axis=-1)
    text_norm = tf.nn.l2_normalize(text_embeddings, axis=-1)
    
    # Similarity matrix
    similarity = tf.matmul(vision_norm, text_norm, transpose_b=True)
    
    # Compute metrics
    # 1. Text-to-Vision Retrieval (R@1, R@5, R@10)
    ranks = tf.argsort(similarity, axis=1, direction='DESCENDING')
    correct_indices = tf.range(tf.shape(ranks)[0])
    
    r1 = tf.reduce_mean(
        tf.cast(tf.equal(ranks[:, 0], correct_indices), tf.float32)
    )
    r5 = tf.reduce_mean(
        tf.cast(tf.reduce_any(tf.equal(ranks[:, :5], correct_indices[:, None]), axis=1), tf.float32)
    )
    r10 = tf.reduce_mean(
        tf.cast(tf.reduce_any(tf.equal(ranks[:, :10], correct_indices[:, None]), axis=1), tf.float32)
    )
    
    return {
        'text_to_vision_R@1': r1.numpy(),
        'text_to_vision_R@5': r5.numpy(),
        'text_to_vision_R@10': r10.numpy(),
        'mean_similarity': tf.reduce_mean(tf.linalg.diag_part(similarity)).numpy()
    }
```

---

## üîß **IMPLEMENTASI PRAKTIS**

### **1. Modifikasi pada preprocess_landmarks.ipynb**

Tambahkan ekstraksi fitur geometris tambahan:

```python
# TAMBAHKAN FUNGSI INI KE NOTEBOOK

def extract_advanced_hand_features(landmarks_3d):
    """
    Ekstrak fitur geometris tingkat tinggi dari landmarks
    
    Args:
        landmarks_3d: (21, 3) array untuk 21 landmarks dengan x,y,z
    
    Returns:
        feature_vector: (N,) array dengan fitur geometris
    """
    features = []
    
    # 1. DISTANCES: Jarak antar landmark penting
    # Finger tips (4, 8, 12, 16, 20) ke wrist (0)
    wrist = landmarks_3d[0]
    finger_tips_idx = [4, 8, 12, 16, 20]
    
    for tip_idx in finger_tips_idx:
        distance = np.linalg.norm(landmarks_3d[tip_idx] - wrist)
        features.append(distance)
    
    # Jarak antar ujung jari
    finger_tips = landmarks_3d[finger_tips_idx]
    for i in range(len(finger_tips)):
        for j in range(i+1, len(finger_tips)):
            dist = np.linalg.norm(finger_tips[i] - finger_tips[j])
            features.append(dist)
    
    # 2. ANGLES: Sudut antar segment jari
    # Untuk setiap jari (thumb, index, middle, ring, pinky)
    finger_chains = [
        [0, 1, 2, 3, 4],      # Thumb
        [0, 5, 6, 7, 8],      # Index
        [0, 9, 10, 11, 12],   # Middle
        [0, 13, 14, 15, 16],  # Ring
        [0, 17, 18, 19, 20]   # Pinky
    ]
    
    for chain in finger_chains:
        for i in range(len(chain) - 2):
            # Hitung sudut antara 3 landmark berurutan
            v1 = landmarks_3d[chain[i+1]] - landmarks_3d[chain[i]]
            v2 = landmarks_3d[chain[i+2]] - landmarks_3d[chain[i+1]]
            
            cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)
            angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))
            features.append(angle)
    
    # 3. PALM ORIENTATION: Normal vector dari palm plane
    # Gunakan 3 landmark: wrist(0), index_mcp(5), pinky_mcp(17)
    palm_points = landmarks_3d[[0, 5, 17]]
    v1 = palm_points[1] - palm_points[0]
    v2 = palm_points[2] - palm_points[0]
    normal = np.cross(v1, v2)
    normal = normal / (np.linalg.norm(normal) + 1e-8)
    features.extend(normal)  # 3 components
    
    # 4. HAND OPENNESS: Rata-rata jarak finger tips ke palm center
    palm_center = np.mean(landmarks_3d[[0, 5, 9, 13, 17]], axis=0)
    finger_tips = landmarks_3d[[4, 8, 12, 16, 20]]
    openness = np.mean([np.linalg.norm(tip - palm_center) for tip in finger_tips])
    features.append(openness)
    
    return np.array(features, dtype=np.float32)

# MODIFIKASI FUNGSI extract_landmarks():
def extract_landmarks_enhanced(image_path):
    """Versi enhanced dengan geometric features"""
    try:
        image = cv2.imread(image_path)
        if image is None:
            return None, None
        
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = hands_model.process(image_rgb)
        
        # Landmarks original (126 dims untuk 2 tangan)
        landmarks_vector = np.zeros(NUM_LANDMARKS, dtype=np.float32)
        
        # Advanced features (per hand)
        advanced_features_list = []
        
        if results.multi_hand_landmarks:
            for i, hand_landmarks in enumerate(results.multi_hand_landmarks):
                handedness = results.multi_handedness[i].classification[0].label
                
                # Original landmarks (relative coords)
                coords = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])
                relative_coords = (coords - coords[0]).flatten()
                
                if handedness == 'Right':
                    landmarks_vector[0:63] = relative_coords
                elif handedness == 'Left':
                    landmarks_vector[63:126] = relative_coords
                
                # Extract advanced features
                advanced_features = extract_advanced_hand_features(coords)
                advanced_features_list.append(advanced_features)
        
        # Concatenate all features
        if advanced_features_list:
            advanced_features_combined = np.concatenate(advanced_features_list)
        else:
            advanced_features_combined = np.zeros(100, dtype=np.float32)  # Placeholder size
        
        # Return both: original landmarks dan advanced features
        return landmarks_vector, advanced_features_combined
        
    except Exception as e:
        print(f"\n‚ö†Ô∏è Error saat ekstrak {image_path}: {e}")
        return None, None
```

### **2. Modifikasi pada Training_Model_Sign.ipynb**

Upgrade arsitektur model:

```python
# ====================================
# CELL BARU: Advanced Model Architecture
# ====================================

# --- KONFIGURASI MODEL ADVANCED ---
USE_ADVANCED_FEATURES = True
USE_CROSS_ATTENTION = True
USE_TEMPORAL_MODELING = False  # Set True jika menggunakan video sequence

# --- BUILD ADVANCED HYBRID MODEL ---
def build_advanced_hybrid_model():
    """
    Arsitektur hybrid advanced dengan:
    1. EfficientNetV2 untuk visual
    2. GCN untuk landmark geometry
    3. Cross-modal attention fusion
    4. Multi-task output (classification + optional translation)
    """
    
    # ============= INPUT LAYERS =============
    # Input 1: Image
    input_image = Input(shape=(*IMAGE_SIZE, 3), name='image_input')
    
    # Input 2: Landmarks (21 landmarks * 3 coords * 2 hands = 126)
    input_landmarks = Input(shape=(NUM_LANDMARKS,), name='landmark_input')
    
    # Input 3 (Optional): Advanced geometric features
    if USE_ADVANCED_FEATURES:
        input_advanced_features = Input(shape=(100,), name='advanced_features_input')
    
    # ============= VISUAL BRANCH =============
    # Data augmentation (only during training)
    augmented_image = data_augmentation(input_image)
    
    # Preprocessing untuk EfficientNetV2
    rescaling_layer = layers.Rescaling(1./127.5, offset=-1)
    preprocessed_image = rescaling_layer(augmented_image)
    
    # EfficientNetV2 (upgrade dari EfficientNetB0)
    base_model_cnn = tf.keras.applications.EfficientNetV2S(
        include_top=False,
        weights='imagenet',
        input_shape=(*IMAGE_SIZE, 3),
        pooling=None  # We'll use custom pooling
    )
    base_model_cnn.trainable = False  # Freeze initially
    
    cnn_features = base_model_cnn(preprocessed_image, training=False)
    
    # Advanced pooling: Spatial Attention + GAP
    # Spatial attention memberikan fokus pada region penting
    attention_scores = layers.Conv2D(1, 1, activation='sigmoid', name='spatial_attention')(cnn_features)
    attended_features = layers.Multiply()([cnn_features, attention_scores])
    visual_features = GlobalAveragePooling2D(name='visual_gap')(attended_features)
    
    # Additional visual processing
    visual_features = Dense(512, activation='relu', name='visual_dense1')(visual_features)
    visual_features = Dropout(0.3)(visual_features)
    visual_features = Dense(256, activation='relu', name='visual_features_final')(visual_features)
    
    # ============= LANDMARK BRANCH =============
    # Reshape landmarks untuk GCN-style processing
    # (batch, 126) -> (batch, 21*2, 3) untuk 2 hands dengan 21 landmarks each
    landmarks_reshaped = layers.Reshape((42, 3), name='reshape_landmarks')(input_landmarks)
    
    # Process dengan Dense layers (simplified GCN)
    # Real GCN requires adjacency matrix (dapat diimplementasikan dengan spektral library)
    x = Dense(128, activation='relu', name='landmark_dense1')(landmarks_reshaped)
    x = Dropout(0.3)(x)
    x = Dense(256, activation='relu', name='landmark_dense2')(x)
    
    # Global pooling untuk landmark features
    landmark_features = GlobalAveragePooling1D(name='landmark_gap')(x)
    
    # Combine dengan advanced features jika ada
    if USE_ADVANCED_FEATURES:
        advanced_proc = Dense(128, activation='relu', name='advanced_dense')(input_advanced_features)
        landmark_features = Concatenate(name='landmark_combined')([landmark_features, advanced_proc])
    
    landmark_features = Dense(256, activation='relu', name='landmark_features_final')(landmark_features)
    
    # ============= CROSS-MODAL FUSION =============
    if USE_CROSS_ATTENTION:
        # Add sequence dimension untuk attention
        visual_seq = tf.expand_dims(visual_features, axis=1)  # (batch, 1, 256)
        landmark_seq = tf.expand_dims(landmark_features, axis=1)  # (batch, 1, 256)
        
        # Cross-attention: visual attends to landmark
        visual_attended = layers.MultiHeadAttention(
            num_heads=4, 
            key_dim=64,
            name='cross_attention_v2l'
        )(query=visual_seq, key=landmark_seq, value=landmark_seq)
        
        # Cross-attention: landmark attends to visual
        landmark_attended = layers.MultiHeadAttention(
            num_heads=4, 
            key_dim=64,
            name='cross_attention_l2v'
        )(query=landmark_seq, key=visual_seq, value=visual_seq)
        
        # Squeeze back to 2D
        visual_attended = tf.squeeze(visual_attended, axis=1)
        landmark_attended = tf.squeeze(landmark_attended, axis=1)
        
        # Combine original + attended features
        visual_final = layers.Add()([visual_features, visual_attended])
        landmark_final = layers.Add()([landmark_features, landmark_attended])
        
        # Concatenate
        combined_features = Concatenate(name='fused_features')([visual_final, landmark_final])
    else:
        # Simple concatenation (original approach)
        combined_features = Concatenate(name='fused_features')([visual_features, landmark_features])
    
    # ============= CLASSIFICATION HEAD =============
    x = Dense(512, activation='relu', name='fusion_dense1')(combined_features)
    x = Dropout(0.5)(x)
    x = Dense(256, activation='relu', name='fusion_dense2')(x)
    x = Dropout(0.4)(x)
    
    output_classification = Dense(
        NUM_CLASSES, 
        activation='softmax', 
        name='classification_output'
    )(x)
    
    # ============= BUILD MODEL =============
    if USE_ADVANCED_FEATURES:
        inputs = [input_image, input_landmarks, input_advanced_features]
    else:
        inputs = [input_image, input_landmarks]
    
    model = Model(
        inputs=inputs,
        outputs=output_classification,
        name='SignBridge_Advanced_Hybrid_Model'
    )
    
    return model

# Build model
model = build_advanced_hybrid_model()
model.summary()

# --- COMPILE MODEL ---
# Menggunakan advanced optimizer
from tensorflow.keras.optimizers import AdamW

# AdamW dengan weight decay untuk regularization
optimizer = AdamW(
    learning_rate=1e-3,
    weight_decay=1e-4,
    clipnorm=1.0  # Gradient clipping
)

model.compile(
    optimizer=optimizer,
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy', tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top5_accuracy')]
)

# --- CALLBACKS ---
from tensorflow.keras.callbacks import (
    EarlyStopping, 
    ReduceLROnPlateau, 
    ModelCheckpoint,
    TensorBoard
)

callbacks = [
    # Early stopping
    EarlyStopping(
        monitor='val_accuracy',
        patience=15,
        restore_best_weights=True,
        verbose=1
    ),
    
    # Learning rate reduction
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-7,
        verbose=1
    ),
    
    # Model checkpoint
    ModelCheckpoint(
        filepath=f'/content/drive/MyDrive/Skripsi/models/{OUTPUT_MODEL_NAME}_best.keras',
        monitor='val_accuracy',
        save_best_only=True,
        verbose=1
    ),
    
    # TensorBoard
    TensorBoard(
        log_dir=f'/content/drive/MyDrive/Skripsi/logs/{OUTPUT_MODEL_NAME}',
        histogram_freq=1
    )
]
```

---

## üìà **EKSPEKTASI PENINGKATAN PERFORMA**

Dengan implementasi rekomendasi di atas, ekspektasi peningkatan:

| **Komponen** | **Baseline** | **Setelah Optimasi** | **Improvement** |
|--------------|--------------|----------------------|-----------------|
| **Classification Accuracy** | 85-90% | 93-97% | +5-10% |
| **Top-5 Accuracy** | 95-97% | 98-99% | +2-3% |
| **Inference Speed (CPU)** | 100ms | 80ms | +20% faster |
| **Inference Speed (GPU)** | 15ms | 10ms | +33% faster |
| **Model Size** | ~25MB | ~40MB | Tradeoff |
| **Zero-shot Capability** | ‚ùå | ‚úÖ | New feature |
| **Translation Quality (BLEU)** | N/A | 30-40 | New feature |

---

## üéì **REFERENSI DAN PEMBELAJARAN LEBIH LANJUT**

### **Papers untuk Dibaca:**
1. **CLIP** (Radford et al., 2021): "Learning Transferable Visual Models From Natural Language Supervision"
2. **ALIGN** (Jia et al., 2021): "Scaling Up Visual and Vision-Language Representation Learning"
3. **ViLT** (Kim et al., 2021): "Vision-and-Language Transformer Without Convolution or Region Supervision"
4. **BLIP** (Li et al., 2022): "Bootstrapping Language-Image Pre-training"
5. **SignBERT** (Hu et al., 2021): "Sign Language Recognition, Generation, and Translation: Survey"

### **Libraries untuk Integrasi:**
```bash
# Core ML
pip install tensorflow==2.14.0
pip install mediapipe==0.10.21

# Vision-Language Models
pip install transformers  # Hugging Face untuk pre-trained models
pip install clip-by-openai  # CLIP model

# NLP Tools
pip install sentencepiece  # Tokenization
pip install sacrebleu  # BLEU score untuk translation

# Visualisasi
pip install tensorboard
pip install wandb  # Weights & Biases untuk experiment tracking

# Spectral Graph Theory (untuk GCN)
pip install spektral  # Graph Neural Networks di Keras
```

### **Pre-trained Models yang Bisa Digunakan:**
1. **EfficientNetV2**: `tf.keras.applications.EfficientNetV2S/M/L`
2. **Vision Transformer (ViT)**: `transformers.ViTModel`
3. **CLIP**: `clip.load("ViT-B/32")`
4. **BERT untuk text encoding**: `transformers.BertModel`

---

## üöÄ **NEXT STEPS: Roadmap Implementasi**

### **Phase 1: Foundation (2-3 minggu)**
‚úÖ Implement EfficientNetV2 upgrade
‚úÖ Implement advanced geometric features extraction
‚úÖ Implement cross-attention fusion
‚úÖ Benchmark against baseline

### **Phase 2: NLP Integration (3-4 minggu)**
‚úÖ Prepare sign-to-text paired dataset
‚úÖ Implement text encoder (BERT/Transformer)
‚úÖ Implement contrastive learning (CLIP-style)
‚úÖ Train and evaluate

### **Phase 3: Advanced Features (2-3 minggu)**
‚úÖ Implement temporal modeling (LSTM/Transformer untuk video)
‚úÖ Implement multi-task learning (classification + translation)
‚úÖ Implement zero-shot learning capability
‚úÖ Comprehensive evaluation

### **Phase 4: Optimization & Deployment (2 minggu)**
‚úÖ Model quantization untuk mobile deployment
‚úÖ Convert to TensorFlow Lite
‚úÖ Optimize inference speed
‚úÖ Build demo application

---

## üìû **KESIMPULAN**

Model hybrid Anda saat ini sudah memiliki **foundation yang solid** dengan:
- ‚úÖ Multi-modal fusion (visual + geometric)
- ‚úÖ Transfer learning dengan EfficientNetB0
- ‚úÖ Efficient landmark preprocessing

Untuk **memaksimalkan** dengan integrasi NLP dan visual:
1. **Upgrade arsitektur** ke cross-attention fusion
2. **Tambahkan text encoder** untuk vision-language alignment
3. **Gunakan contrastive learning** untuk zero-shot capability
4. **Implement multi-task learning** untuk robust features

**Prioritas tertinggi:**
- [ ] Implement cross-modal attention fusion (biggest impact, moderate effort)
- [ ] Upgrade ke EfficientNetV2 (easy, good improvement)
- [ ] Add advanced geometric features (easy, moderate improvement)
- [ ] Prepare paired sign-text dataset (time-consuming but essential for NLP integration)

Good luck dengan optimasi model Anda! üöÄ

---

**Dokumen ini dibuat untuk:** Optimasi Model Hybrid Sign Language Recognition  
**Tanggal:** 2025-11-10  
**Versi:** 1.0  
