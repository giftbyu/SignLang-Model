{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe --q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bEhQnENaCB5",
        "outputId": "511ccfb3-ed46-45bd-a24b-06fcbce35d36"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.2/81.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JUllEc1ZDss",
        "outputId": "5c716145-7976-4637-cd6b-c0b400aafb53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.7.2 is installed, but it is not compatible with the installed jaxlib version 0.7.1, so it will not be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "âœ… Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import shutil\n",
        "import time\n",
        "from pathlib import Path\n",
        "import google.colab.drive as drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"âœ… Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "METODE = 'BISINDO'  # Pilih 'SIBI' atau 'BISINDO'\n",
        "\n",
        "if METODE == 'SIBI':\n",
        "    INPUT_DIR = '/content/drive/MyDrive/Skripsi/dataset/SIBI_augmentend'\n",
        "    DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/Skripsi/dataset/SIBI_landmarks_v2'\n",
        "    MAX_HANDS = 1\n",
        "else:\n",
        "    INPUT_DIR = '/content/drive/MyDrive/Skripsi/dataset/BISINDO'\n",
        "    DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/Skripsi/dataset/BISINDO_landmarks_v2'\n",
        "    MAX_HANDS = 2\n",
        "\n",
        "# Simpan ke local dulu (lebih cepat)\n",
        "LOCAL_OUTPUT_DIR = '/content/temp_landmarks_v2'\n",
        "CHECKPOINT_FILE = '/content/checkpoint_landmarks_v2.pkl'\n",
        "\n",
        "# Konfigurasi landmarks\n",
        "NUM_LANDMARKS_PER_HAND = 21\n",
        "NUM_COORDS = 3  # x, y, z\n",
        "TOTAL_LANDMARK_FEATURES = NUM_LANDMARKS_PER_HAND * NUM_COORDS * 2  # 126 (untuk 2 tangan)\n",
        "\n",
        "# Konfigurasi advanced features\n",
        "# Per hand: 5 (finger-wrist) + 10 (inter-finger) + 15 (angles) + 3 (palm normal) + 1 (openness) = 34\n",
        "ADVANCED_FEATURES_PER_HAND = 34\n",
        "TOTAL_ADVANCED_FEATURES = ADVANCED_FEATURES_PER_HAND * 2  # 68 (untuk 2 tangan)\n",
        "\n",
        "CHECKPOINT_INTERVAL = 50  # Save checkpoint setiap N files\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"ğŸš€ PREPROCESSING CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Metode: {METODE}\")\n",
        "print(f\"Max Hands: {MAX_HANDS}\")\n",
        "print(f\"Input Directory: {INPUT_DIR}\")\n",
        "print(f\"Output Directory: {DRIVE_OUTPUT_DIR}\")\n",
        "print(f\"Local Temp: {LOCAL_OUTPUT_DIR}\")\n",
        "print(f\"\\nğŸ“Š Feature Configuration:\")\n",
        "print(f\"  â€¢ Basic Landmarks: {TOTAL_LANDMARK_FEATURES} features\")\n",
        "print(f\"  â€¢ Advanced Features: {TOTAL_ADVANCED_FEATURES} features\")\n",
        "print(f\"  â€¢ Total per sample: {TOTAL_LANDMARK_FEATURES + TOTAL_ADVANCED_FEATURES} features\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vObG4WDiZFfI",
        "outputId": "0889063b-b2c6-4299-8ad2-6285cc0c641d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ğŸš€ PREPROCESSING CONFIGURATION\n",
            "======================================================================\n",
            "Metode: BISINDO\n",
            "Max Hands: 2\n",
            "Input Directory: /content/drive/MyDrive/Skripsi/dataset/BISINDO\n",
            "Output Directory: /content/drive/MyDrive/Skripsi/dataset/BISINDO_landmarks_v2\n",
            "Local Temp: /content/temp_landmarks_v2\n",
            "\n",
            "ğŸ“Š Feature Configuration:\n",
            "  â€¢ Basic Landmarks: 126 features\n",
            "  â€¢ Advanced Features: 68 features\n",
            "  â€¢ Total per sample: 194 features\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mp_hands = mp.solutions.hands\n",
        "hands_model = mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=MAX_HANDS,\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5\n",
        ")\n",
        "\n",
        "print(\"âœ… MediaPipe Hands model initialized\")\n",
        "print(f\"   - Static image mode: True\")\n",
        "print(f\"   - Max hands: {MAX_HANDS}\")\n",
        "print(f\"   - Min detection confidence: 0.5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVOOSeUOZTQW",
        "outputId": "0732a14d-2403-46bc-843f-e307256dd6dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… MediaPipe Hands model initialized\n",
            "   - Static image mode: True\n",
            "   - Max hands: 2\n",
            "   - Min detection confidence: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_advanced_hand_features(landmarks_3d):\n",
        "    \"\"\"\n",
        "    Ekstrak fitur geometris tingkat tinggi dari hand landmarks.\n",
        "\n",
        "    Args:\n",
        "        landmarks_3d: numpy array dengan shape (21, 3) untuk x,y,z coordinates\n",
        "\n",
        "    Returns:\n",
        "        feature_vector: numpy array dengan ~34 features per hand\n",
        "\n",
        "    Features yang diekstrak:\n",
        "        1. Finger tip to wrist distances (5 features)\n",
        "        2. Inter-finger distances (10 features)\n",
        "        3. Joint angles (15 features)\n",
        "        4. Palm orientation normal vector (3 features)\n",
        "        5. Hand openness (1 feature)\n",
        "        Total: 34 features per hand\n",
        "    \"\"\"\n",
        "    features = []\n",
        "\n",
        "    try:\n",
        "        # 1. FINGER TIP TO WRIST DISTANCES (5 features)\n",
        "        wrist = landmarks_3d[0]\n",
        "        finger_tips_idx = [4, 8, 12, 16, 20]  # Thumb, Index, Middle, Ring, Pinky\n",
        "\n",
        "        for tip_idx in finger_tips_idx:\n",
        "            distance = np.linalg.norm(landmarks_3d[tip_idx] - wrist)\n",
        "            features.append(distance)\n",
        "\n",
        "        # 2. INTER-FINGER DISTANCES (10 features)\n",
        "        # Jarak antar ujung jari (C(5,2) = 10 combinations)\n",
        "        finger_tips = landmarks_3d[finger_tips_idx]\n",
        "        for i in range(len(finger_tips)):\n",
        "            for j in range(i+1, len(finger_tips)):\n",
        "                dist = np.linalg.norm(finger_tips[i] - finger_tips[j])\n",
        "                features.append(dist)\n",
        "\n",
        "        # 3. JOINT ANGLES (15 features = 5 fingers Ã— 3 joints each)\n",
        "        # Untuk setiap jari, hitung angle di setiap joint\n",
        "        finger_chains = [\n",
        "            [0, 1, 2, 3, 4],      # Thumb\n",
        "            [0, 5, 6, 7, 8],      # Index\n",
        "            [0, 9, 10, 11, 12],   # Middle\n",
        "            [0, 13, 14, 15, 16],  # Ring\n",
        "            [0, 17, 18, 19, 20]   # Pinky\n",
        "        ]\n",
        "\n",
        "        for chain in finger_chains:\n",
        "            for i in range(len(chain) - 2):\n",
        "                # Vector dari joint i ke i+1\n",
        "                v1 = landmarks_3d[chain[i+1]] - landmarks_3d[chain[i]]\n",
        "                # Vector dari joint i+1 ke i+2\n",
        "                v2 = landmarks_3d[chain[i+2]] - landmarks_3d[chain[i+1]]\n",
        "\n",
        "                # Compute angle menggunakan dot product\n",
        "                v1_norm = np.linalg.norm(v1)\n",
        "                v2_norm = np.linalg.norm(v2)\n",
        "\n",
        "                if v1_norm > 1e-6 and v2_norm > 1e-6:\n",
        "                    cos_angle = np.dot(v1, v2) / (v1_norm * v2_norm)\n",
        "                    cos_angle = np.clip(cos_angle, -1.0, 1.0)\n",
        "                    angle = np.arccos(cos_angle)\n",
        "                else:\n",
        "                    angle = 0.0\n",
        "\n",
        "                features.append(angle)\n",
        "\n",
        "        # 4. PALM ORIENTATION (3 features)\n",
        "        # Normal vector dari palm plane\n",
        "        # Gunakan 3 landmark: wrist(0), index_mcp(5), pinky_mcp(17)\n",
        "        palm_points = landmarks_3d[[0, 5, 17]]\n",
        "        v1 = palm_points[1] - palm_points[0]\n",
        "        v2 = palm_points[2] - palm_points[0]\n",
        "\n",
        "        # Cross product untuk normal vector\n",
        "        normal = np.cross(v1, v2)\n",
        "        normal_norm = np.linalg.norm(normal)\n",
        "\n",
        "        if normal_norm > 1e-6:\n",
        "            normal = normal / normal_norm\n",
        "        else:\n",
        "            normal = np.array([0.0, 0.0, 1.0])  # Default\n",
        "\n",
        "        features.extend(normal)  # 3 components\n",
        "\n",
        "        # 5. HAND OPENNESS (1 feature)\n",
        "        # Rata-rata jarak finger tips ke palm center\n",
        "        palm_center = np.mean(landmarks_3d[[0, 5, 9, 13, 17]], axis=0)\n",
        "        openness = np.mean([np.linalg.norm(tip - palm_center) for tip in finger_tips])\n",
        "        features.append(openness)\n",
        "\n",
        "        return np.array(features, dtype=np.float32)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Return zeros jika ada error\n",
        "        print(f\"âš ï¸  Error in advanced feature extraction: {e}\")\n",
        "        return np.zeros(ADVANCED_FEATURES_PER_HAND, dtype=np.float32)\n",
        "\n",
        "\n",
        "def validate_advanced_features(features):\n",
        "    \"\"\"\n",
        "    Validasi advanced features (check for NaN, Inf, etc.)\n",
        "    \"\"\"\n",
        "    # Replace NaN with 0\n",
        "    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    # Clip extreme values\n",
        "    features = np.clip(features, -10.0, 10.0)\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "print(\"âœ… Advanced feature extraction functions loaded\")\n",
        "print(f\"   Expected features per hand: {ADVANCED_FEATURES_PER_HAND}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-H5OMEdZVPX",
        "outputId": "fd6114fa-1983-41a0-f8bf-6db7394bf9d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Advanced feature extraction functions loaded\n",
            "   Expected features per hand: 34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 5: MAIN EXTRACTION FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def extract_landmarks_and_features(image_path):\n",
        "    \"\"\"\n",
        "    Ekstrak basic landmarks DAN advanced features dari image.\n",
        "\n",
        "    Args:\n",
        "        image_path: Path ke image file\n",
        "\n",
        "    Returns:\n",
        "        landmarks_vector: Array dengan shape (126,) untuk basic landmarks\n",
        "        advanced_features: Array dengan shape (68,) untuk advanced features\n",
        "\n",
        "    Return None, None jika gagal.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read image\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            return None, None\n",
        "\n",
        "        # Convert to RGB\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Process dengan MediaPipe\n",
        "        results = hands_model.process(image_rgb)\n",
        "\n",
        "        # Initialize output arrays\n",
        "        landmarks_vector = np.zeros(TOTAL_LANDMARK_FEATURES, dtype=np.float32)\n",
        "        advanced_features_combined = np.zeros(TOTAL_ADVANCED_FEATURES, dtype=np.float32)\n",
        "\n",
        "        # Extract jika ada tangan terdeteksi\n",
        "        if results.multi_hand_landmarks:\n",
        "            for i, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
        "                # Get handedness\n",
        "                handedness = results.multi_handedness[i].classification[0].label\n",
        "\n",
        "                # Extract coordinates\n",
        "                coords = np.array([\n",
        "                    [lm.x, lm.y, lm.z]\n",
        "                    for lm in hand_landmarks.landmark\n",
        "                ])  # Shape: (21, 3)\n",
        "\n",
        "                # === BASIC LANDMARKS (relative to wrist) ===\n",
        "                relative_coords = (coords - coords[0]).flatten()  # Shape: (63,)\n",
        "\n",
        "                if handedness == 'Right':\n",
        "                    landmarks_vector[0:63] = relative_coords\n",
        "                    hand_idx = 0\n",
        "                elif handedness == 'Left':\n",
        "                    landmarks_vector[63:126] = relative_coords\n",
        "                    hand_idx = 1\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                # === ADVANCED FEATURES ===\n",
        "                advanced_features = extract_advanced_hand_features(coords)\n",
        "                advanced_features = validate_advanced_features(advanced_features)\n",
        "\n",
        "                # Store in appropriate position\n",
        "                start_idx = hand_idx * ADVANCED_FEATURES_PER_HAND\n",
        "                end_idx = start_idx + ADVANCED_FEATURES_PER_HAND\n",
        "                advanced_features_combined[start_idx:end_idx] = advanced_features\n",
        "\n",
        "        return landmarks_vector, advanced_features_combined\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Error processing {os.path.basename(image_path)}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "print(\"âœ… Main extraction function loaded\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d44tHkyUZZ2b",
        "outputId": "25fb50f7-86df-4a75-a343-eb4626d4e429"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Main extraction function loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 6: CHECKPOINT FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def save_checkpoint(processed_files):\n",
        "    \"\"\"Simpan checkpoint ke file.\"\"\"\n",
        "    try:\n",
        "        with open(CHECKPOINT_FILE, 'wb') as f:\n",
        "            pickle.dump(processed_files, f)\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Warning: Failed to save checkpoint: {e}\")\n",
        "\n",
        "\n",
        "def load_checkpoint():\n",
        "    \"\"\"Load checkpoint jika ada.\"\"\"\n",
        "    if os.path.exists(CHECKPOINT_FILE):\n",
        "        try:\n",
        "            with open(CHECKPOINT_FILE, 'rb') as f:\n",
        "                return pickle.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸  Warning: Failed to load checkpoint: {e}\")\n",
        "            return set()\n",
        "    return set()\n",
        "\n",
        "\n",
        "def save_with_retry(output_path, data, max_retries=3):\n",
        "    \"\"\"Simpan file dengan retry mechanism.\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "            np.save(output_path, data)\n",
        "            return True\n",
        "        except (ConnectionAbortedError, OSError, IOError) as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2)\n",
        "            else:\n",
        "                print(f\"âŒ Failed to save {os.path.basename(output_path)} after {max_retries} attempts\")\n",
        "                return False\n",
        "    return False\n",
        "\n",
        "\n",
        "print(\"âœ… Checkpoint functions loaded\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mgLrMViZcjw",
        "outputId": "e2a3004f-6d4e-477a-ee14-b76e7ad69043"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Checkpoint functions loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 7: MAIN PROCESSING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸš€ STARTING LANDMARK EXTRACTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(LOCAL_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Get all image paths\n",
        "image_paths = sorted(glob.glob(os.path.join(INPUT_DIR, '*/*.jpg')))\n",
        "if not image_paths:\n",
        "    raise ValueError(f\"âŒ No images found in {INPUT_DIR}\")\n",
        "\n",
        "print(f\"\\nğŸ“Š Dataset Statistics:\")\n",
        "print(f\"  â€¢ Total images found: {len(image_paths)}\")\n",
        "\n",
        "# Load checkpoint if exists\n",
        "processed_files = load_checkpoint()\n",
        "if processed_files:\n",
        "    print(f\"  â€¢ Resuming from checkpoint: {len(processed_files)} files already processed\")\n",
        "    image_paths = [p for p in image_paths if p not in processed_files]\n",
        "    print(f\"  â€¢ Remaining to process: {len(image_paths)}\")\n",
        "\n",
        "# Statistics\n",
        "stats = {\n",
        "    'total': len(image_paths) + len(processed_files),\n",
        "    'processed': len(processed_files),\n",
        "    'success': 0,\n",
        "    'failed': 0,\n",
        "    'no_hand_detected': 0\n",
        "}\n",
        "\n",
        "failed_files = []\n",
        "\n",
        "print(f\"\\nğŸ”„ Processing images...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Process each image\n",
        "for idx, image_path in enumerate(tqdm(image_paths, desc=\"Extracting Features\")):\n",
        "    # Extract features\n",
        "    landmarks, advanced_features = extract_landmarks_and_features(image_path)\n",
        "\n",
        "    if landmarks is not None:\n",
        "        # Create output paths\n",
        "        relative_path = os.path.relpath(image_path, INPUT_DIR)\n",
        "        base_output_path = os.path.join(LOCAL_OUTPUT_DIR, relative_path)\n",
        "        base_output_path = os.path.splitext(base_output_path)[0]\n",
        "\n",
        "        # Save basic landmarks\n",
        "        landmarks_path = base_output_path + '_landmarks.npy'\n",
        "        success_landmarks = save_with_retry(landmarks_path, landmarks)\n",
        "\n",
        "        # Save advanced features\n",
        "        advanced_path = base_output_path + '_advanced.npy'\n",
        "        success_advanced = save_with_retry(advanced_path, advanced_features)\n",
        "\n",
        "        if success_landmarks and success_advanced:\n",
        "            processed_files.add(image_path)\n",
        "            stats['processed'] += 1\n",
        "            stats['success'] += 1\n",
        "\n",
        "            # Check if hand was detected\n",
        "            if np.sum(np.abs(landmarks)) < 0.001:\n",
        "                stats['no_hand_detected'] += 1\n",
        "        else:\n",
        "            failed_files.append(image_path)\n",
        "            stats['failed'] += 1\n",
        "    else:\n",
        "        failed_files.append(image_path)\n",
        "        stats['failed'] += 1\n",
        "\n",
        "    # Save checkpoint periodically\n",
        "    if (idx + 1) % CHECKPOINT_INTERVAL == 0:\n",
        "        save_checkpoint(processed_files)\n",
        "\n",
        "# Final checkpoint\n",
        "save_checkpoint(processed_files)\n",
        "\n",
        "# Close MediaPipe\n",
        "hands_model.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_Nm959oZjus",
        "outputId": "3bb465d1-2136-4783-fa2b-1228d97cdd1d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "ğŸš€ STARTING LANDMARK EXTRACTION\n",
            "======================================================================\n",
            "\n",
            "ğŸ“Š Dataset Statistics:\n",
            "  â€¢ Total images found: 10400\n",
            "\n",
            "ğŸ”„ Processing images...\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10400/10400 [2:09:12<00:00,  1.34it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 8: PROCESSING SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ“Š PROCESSING SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total images: {stats['total']}\")\n",
        "print(f\"Successfully processed: {stats['success']}\")\n",
        "print(f\"Failed: {stats['failed']}\")\n",
        "print(f\"No hand detected: {stats['no_hand_detected']}\")\n",
        "print(f\"Success rate: {stats['success']/stats['total']*100:.2f}%\")\n",
        "\n",
        "if failed_files:\n",
        "    print(f\"\\nâš ï¸  {len(failed_files)} files failed to process:\")\n",
        "    for f in failed_files[:10]:\n",
        "        print(f\"   - {os.path.basename(f)}\")\n",
        "    if len(failed_files) > 10:\n",
        "        print(f\"   ... and {len(failed_files) - 10} more\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtarWc9CZl5-",
        "outputId": "4f93b666-3209-441b-d10f-80d8bb6af0d8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ğŸ“Š PROCESSING SUMMARY\n",
            "======================================================================\n",
            "Total images: 10400\n",
            "Successfully processed: 10400\n",
            "Failed: 0\n",
            "No hand detected: 85\n",
            "Success rate: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 9: VERIFY OUTPUT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ” VERIFYING OUTPUT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Count output files\n",
        "landmark_files = glob.glob(os.path.join(LOCAL_OUTPUT_DIR, '**/*_landmarks.npy'), recursive=True)\n",
        "advanced_files = glob.glob(os.path.join(LOCAL_OUTPUT_DIR, '**/*_advanced.npy'), recursive=True)\n",
        "\n",
        "print(f\"Landmark files: {len(landmark_files)}\")\n",
        "print(f\"Advanced feature files: {len(advanced_files)}\")\n",
        "\n",
        "# Verify a sample file\n",
        "if landmark_files and advanced_files:\n",
        "    sample_landmarks = np.load(landmark_files[0])\n",
        "    sample_advanced = np.load(advanced_files[0])\n",
        "\n",
        "    print(f\"\\nâœ… Sample verification:\")\n",
        "    print(f\"  â€¢ Landmarks shape: {sample_landmarks.shape} (expected: ({TOTAL_LANDMARK_FEATURES},))\")\n",
        "    print(f\"  â€¢ Advanced features shape: {sample_advanced.shape} (expected: ({TOTAL_ADVANCED_FEATURES},))\")\n",
        "    print(f\"  â€¢ Landmarks range: [{sample_landmarks.min():.4f}, {sample_landmarks.max():.4f}]\")\n",
        "    print(f\"  â€¢ Advanced range: [{sample_advanced.min():.4f}, {sample_advanced.max():.4f}]\")\n",
        "\n",
        "    # Check for anomalies\n",
        "    has_nan_landmarks = np.isnan(sample_landmarks).any()\n",
        "    has_nan_advanced = np.isnan(sample_advanced).any()\n",
        "\n",
        "    if has_nan_landmarks or has_nan_advanced:\n",
        "        print(\"  âš ï¸  WARNING: NaN values detected!\")\n",
        "    else:\n",
        "        print(\"  âœ… No NaN values detected\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwe7eXhOZpRl",
        "outputId": "f3b2cc60-cb52-4e24-a091-75d7c7d93193"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ğŸ” VERIFYING OUTPUT\n",
            "======================================================================\n",
            "Landmark files: 10400\n",
            "Advanced feature files: 10400\n",
            "\n",
            "âœ… Sample verification:\n",
            "  â€¢ Landmarks shape: (126,) (expected: (126,))\n",
            "  â€¢ Advanced features shape: (68,) (expected: (68,))\n",
            "  â€¢ Landmarks range: [-0.6147, 0.1193]\n",
            "  â€¢ Advanced range: [-0.9538, 2.0903]\n",
            "  âœ… No NaN values detected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 10: COPY TO GOOGLE DRIVE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ“¦ COPYING TO GOOGLE DRIVE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"From: {LOCAL_OUTPUT_DIR}\")\n",
        "print(f\"To:   {DRIVE_OUTPUT_DIR}\")\n",
        "\n",
        "try:\n",
        "    # Create Drive directory if not exists\n",
        "    os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    # Copy tree\n",
        "    total_files = len(landmark_files) + len(advanced_files)\n",
        "    print(f\"Total files to copy: {total_files}\")\n",
        "\n",
        "    print(\"Copying... (this may take a few minutes)\")\n",
        "    shutil.copytree(LOCAL_OUTPUT_DIR, DRIVE_OUTPUT_DIR, dirs_exist_ok=True)\n",
        "\n",
        "    print(\"âœ… Files successfully copied to Google Drive!\")\n",
        "\n",
        "    # Cleanup local files\n",
        "    print(\"\\nğŸ§¹ Cleaning up local temporary files...\")\n",
        "    shutil.rmtree(LOCAL_OUTPUT_DIR)\n",
        "    if os.path.exists(CHECKPOINT_FILE):\n",
        "        os.remove(CHECKPOINT_FILE)\n",
        "\n",
        "    print(\"âœ… Cleanup completed!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Error copying to Drive: {e}\")\n",
        "    print(f\"âš ï¸  Files are still available at: {LOCAL_OUTPUT_DIR}\")\n",
        "    print(\"ğŸ’¡ You can manually copy them or retry later\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Jms7jHWZuVp",
        "outputId": "8cb37eff-5b79-474f-aa6d-a5d4067da325"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ğŸ“¦ COPYING TO GOOGLE DRIVE\n",
            "======================================================================\n",
            "From: /content/temp_landmarks_v2\n",
            "To:   /content/drive/MyDrive/Skripsi/dataset/BISINDO_landmarks_v2\n",
            "Total files to copy: 20800\n",
            "Copying... (this may take a few minutes)\n",
            "âœ… Files successfully copied to Google Drive!\n",
            "\n",
            "ğŸ§¹ Cleaning up local temporary files...\n",
            "âœ… Cleanup completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ‰ PREPROCESSING COMPLETED!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nğŸ“ Output Structure:\")\n",
        "print(f\"{DRIVE_OUTPUT_DIR}/\")\n",
        "print(\"  â”œâ”€â”€ ClassA/\")\n",
        "print(\"  â”‚   â”œâ”€â”€ image001_landmarks.npy    (126 features)\")\n",
        "print(\"  â”‚   â”œâ”€â”€ image001_advanced.npy     (68 features)\")\n",
        "print(\"  â”‚   â””â”€â”€ ...\")\n",
        "print(\"  â””â”€â”€ ClassB/\")\n",
        "print(\"      â””â”€â”€ ...\")\n",
        "\n",
        "print(\"\\nğŸ“Š Feature Summary:\")\n",
        "print(f\"  â€¢ Basic Landmarks: {TOTAL_LANDMARK_FEATURES} features per sample\")\n",
        "print(f\"    - Right hand: 63 (21 landmarks Ã— 3 coords)\")\n",
        "print(f\"    - Left hand:  63 (21 landmarks Ã— 3 coords)\")\n",
        "print(f\"\\n  â€¢ Advanced Features: {TOTAL_ADVANCED_FEATURES} features per sample\")\n",
        "print(f\"    - Right hand: {ADVANCED_FEATURES_PER_HAND} (geometric features)\")\n",
        "print(f\"    - Left hand:  {ADVANCED_FEATURES_PER_HAND} (geometric features)\")\n",
        "print(f\"\\n  â€¢ TOTAL: {TOTAL_LANDMARK_FEATURES + TOTAL_ADVANCED_FEATURES} features per sample\")\n",
        "\n",
        "print(\"\\nâœ… Next Steps:\")\n",
        "print(\"  1. Update data loading function in training notebook\")\n",
        "print(\"  2. Load both *_landmarks.npy and *_advanced.npy\")\n",
        "print(\"  3. Concatenate or use as separate inputs\")\n",
        "print(\"  4. Train optimized model\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸš€ Ready for training with enhanced features!\")\n",
        "print(\"=\"*70)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl2ET8PwZusj",
        "outputId": "4ee65c54-b2b2-4373-f2b4-108f07503199"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ğŸ‰ PREPROCESSING COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "ğŸ“ Output Structure:\n",
            "/content/drive/MyDrive/Skripsi/dataset/BISINDO_landmarks_v2/\n",
            "  â”œâ”€â”€ ClassA/\n",
            "  â”‚   â”œâ”€â”€ image001_landmarks.npy    (126 features)\n",
            "  â”‚   â”œâ”€â”€ image001_advanced.npy     (68 features)\n",
            "  â”‚   â””â”€â”€ ...\n",
            "  â””â”€â”€ ClassB/\n",
            "      â””â”€â”€ ...\n",
            "\n",
            "ğŸ“Š Feature Summary:\n",
            "  â€¢ Basic Landmarks: 126 features per sample\n",
            "    - Right hand: 63 (21 landmarks Ã— 3 coords)\n",
            "    - Left hand:  63 (21 landmarks Ã— 3 coords)\n",
            "\n",
            "  â€¢ Advanced Features: 68 features per sample\n",
            "    - Right hand: 34 (geometric features)\n",
            "    - Left hand:  34 (geometric features)\n",
            "\n",
            "  â€¢ TOTAL: 194 features per sample\n",
            "\n",
            "âœ… Next Steps:\n",
            "  1. Update data loading function in training notebook\n",
            "  2. Load both *_landmarks.npy and *_advanced.npy\n",
            "  3. Concatenate or use as separate inputs\n",
            "  4. Train optimized model\n",
            "\n",
            "======================================================================\n",
            "ğŸš€ Ready for training with enhanced features!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f8Ex_q2PzGB0"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}